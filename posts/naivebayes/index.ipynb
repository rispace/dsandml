{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification using Naive Bayes algorithm\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-10-10\n",
        "\n",
        "## Introduction"
      ],
      "id": "79c4f689-5d28-4ebd-bbe5-700baae152de"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "9374a2d7-e82d-4bf7-8f1a-6a7c6fd782ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Naive Bayes is a family of simple yet powerful probabilistic classifiers\n",
        "based on Bayes’ Theorem, with the assumption of independence among\n",
        "predictors. It is widely used for tasks like spam detection, text\n",
        "classification, and sentiment analysis due to its efficiency and\n",
        "simplicity. Despite being called “naive” for its strong assumption of\n",
        "feature independence, it often performs remarkably well in real-world\n",
        "scenarios."
      ],
      "id": "ba81bf9b-74a2-4694-8e32-c21998f555c7"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "3b6766c9-10d6-4275-8a17-e0e308f17faf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Naive Bayes?"
      ],
      "id": "ab94e611-21d7-4616-81cc-6820210d3d84"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "95ce378a-5838-4438-adbf-d50aa3a8429e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Naive Bayes is a probabilistic classifier that leverages Bayes’ Theorem\n",
        "to predict the class of a given data point. It belongs to the family of\n",
        "generative models and works by estimating the posterior probability of a\n",
        "class given a set of features. The term “Naive” refers to the assumption\n",
        "that features are conditionally independent given the class label, which\n",
        "simplifies computation."
      ],
      "id": "cfbc2979-f1bf-4806-af80-4762e2724a0d"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "4c32da59-ddea-488f-a55d-307ef75a490b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bayes’ Theorem: The Foundation\n",
        "\n",
        "Bayes’ Theorem provides a way to update our beliefs about the\n",
        "probability of an event, based on new evidence. The formula for Bayes’\n",
        "Theorem is:\n",
        "\n",
        "$$\n",
        "P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $P(y|X)$: Posterior probability of class $y$ given feature set $X$  \n",
        "-   $P(X|y)$: Likelihood of feature set $X$ given class $y$  \n",
        "-   $P(y)$: Prior probability of class $y$  \n",
        "-   $P(X)$: Evidence or probability of feature set $X$\n",
        "\n",
        "In the context of classification:\n",
        "\n",
        "-   The goal is to predict $y$ (the class) given $X$ (the features).  \n",
        "-   $P(y)$ is derived from the distribution of classes in the training\n",
        "    data.  \n",
        "-   $P(X|y)$ is derived from the distribution of features for each\n",
        "    class.  \n",
        "-   $P(X)$ is a normalizing constant to ensure probabilities sum to 1,\n",
        "    but it can be ignored for classification purposes because it is the\n",
        "    same for all classes.\n",
        "\n",
        "### Assumptions and Requirements\n",
        "\n",
        "The key assumption in Naive Bayes is the **conditional independence** of\n",
        "features. Specifically, it assumes that the likelihood of each feature\n",
        "is independent of the others, given the class label:\n",
        "\n",
        "$$\n",
        "P(X_1, X_2, \\dots, X_n | y) = P(X_1 | y) \\cdot P(X_2 | y) \\cdot \\dots \\cdot P(X_n | y)\n",
        "$$\n",
        "\n",
        "While this assumption is often violated in real-world data, Naive Bayes\n",
        "can still perform well, especially when certain features dominate the\n",
        "prediction.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "-   **Numerical Data**: Naive Bayes can handle both numerical and\n",
        "    categorical data, though different versions (Gaussian, Multinomial,\n",
        "    Bernoulli) of the algorithm handle specific types of data more\n",
        "    effectively\n",
        "-   **Non-Collinear Features**: Highly correlated features can distort\n",
        "    predictions since the model assumes independence.  \n",
        "-   **Sufficient Data**: Naive Bayes relies on probability estimates;\n",
        "    thus, insufficient data might lead to unreliable predictions.\n",
        "\n",
        "## Types of Naive Bayes Classifiers\n",
        "\n",
        "There are several variants of Naive Bayes, depending on the nature of\n",
        "the data:\n",
        "\n",
        "1.  **Gaussian Naive Bayes**: Assumes features follow a Gaussian\n",
        "    distribution (useful for continuous data).  \n",
        "2.  **Multinomial Naive Bayes**: Suitable for discrete data, often used\n",
        "    in text classification (e.g., word counts).  \n",
        "3.  **Bernoulli Naive Bayes**: Works well for binary/boolean data, often\n",
        "    used in scenarios where the features represent the presence/absence\n",
        "    of a characteristic.\n",
        "\n",
        "## Mathematics behind the process\n",
        "\n",
        "To understand the working of Naive Bayes, let’s start with the Bayes’s\n",
        "theorem\n",
        "\n",
        "$$\n",
        "P(y_k | X) = \\frac{P(X|y_k) \\cdot P(y_k)}{P(X)}\n",
        "$$\n",
        "\n",
        "Where $y_k$ is one of the possible classes. Due to the independence\n",
        "assumption, the likelihood term $P(X|y_k)$ can be factorized as:\n",
        "\n",
        "$$\n",
        "P(X|y_k) = P(x_1|y_k) \\cdot P(x_2|y_k) \\cdot \\dots \\cdot P(x_n|y_k)\n",
        "$$\n",
        "\n",
        "Where $x_1, x_2, \\dots, x_n$ are the individual features in the feature\n",
        "set $X$. For each class $y_k$, compute the posterior probability:\n",
        "\n",
        "$$\n",
        "P(y_k | X) \\propto P(y_k) \\cdot \\prod_{i=1}^n P(x_i|y_k)\n",
        "$$\n",
        "\n",
        "The denominator $P(X)$ is constant for all classes, so we can ignore it\n",
        "during classification. Finally, the class $y_k$ with the highest\n",
        "posterior probability is chosen as the predicted class:\n",
        "\n",
        "### Computing the probabilities\n",
        "\n",
        "#### Prior Probabilities\n",
        "\n",
        "$P(y_k)$ is the prior probability, usually frequency of each class\n",
        "$k$.  \n",
        "$$\n",
        "  P(y_k)=\\frac{\\text{number of instances in class }y_k}{\\text{total number of instances}}\n",
        "$$\n",
        "\n",
        "#### Class Conditional Probabilities\n",
        "\n",
        "$P(x_i|y_k)$ is the class conditional probability. For the\n",
        "\n",
        "1.  ***Gaussian Naive Bayes:*** when the features are continuous and\n",
        "    assumed that the features follow a ***Gaussian*** distribution, the\n",
        "    `class conditional` probability is given as $$\n",
        "     P(x_i|y_k) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_k}}\\exp{\\left(-\\frac{(x_i-\\mu_i)^2}{2\\sigma^2_k}\\right)}\n",
        "    $$\n",
        "\n",
        "2.  ***Multinomial Naive Bayes:*** when the featrues (typically word\n",
        "    frequencies) follow a multinomial distribution, the\n",
        "    `class conditional` distribution is given as  \n",
        "    $$\n",
        "     P(x_i|y_k)=\\frac{N_{x_i,y_k}+\\alpha}{N_{y_k}+\\alpha V}\n",
        "     $$\n",
        "\n",
        "    where,\n",
        "\n",
        "    -   $N_{x_i,y_k}$ is the count of the feature (e.g. word or term)\n",
        "        $x_i$ appearing in documents of class $y_k$  \n",
        "    -   $N_{y_k}$ is the total count of all features (e.g. words) in all\n",
        "        documents belonging to class $y_k$  \n",
        "    -   $\\alpha$ is a smoothing parameter (often called **Laplace\n",
        "        smoothing**), used to avoid zero probabilities. If not using\n",
        "        smoothing, set $\\alpha=0$  \n",
        "    -   $V$ is the size of the vocabulary (i.e., the number of unique\n",
        "        words)\n",
        "\n",
        "3.  ***Bernoulli Naive Bayes:*** when features are binary/boolean data,\n",
        "    often used in scenarios where the features represent the\n",
        "    presence/absence of a characteristic, the `class conditional`\n",
        "    distribution is given as  \n",
        "    $$\n",
        "     P(x_i|y_k)=\\begin{cases}\\frac{N_{x_i,y_k}+\\alpha}{N_{y_k}+2\\alpha }\\hspace{2mm}\\text{ if } x_i=1\\\\\n",
        "     1-\\frac{N_{x_i,y_k}+\\alpha}{N_{y_k}+2\\alpha }\\hspace{2mm}\\text{ if } x_i=0\\end{cases}\n",
        "    $$\n",
        "\n",
        "## Python Implementation\n",
        "\n",
        "### Gaussian Naive Bayes\n",
        "\n",
        "Code credit for the custom classifier goes to <a\n",
        "href=\"https://github.com/AssemblyAI-Community/Machine-Learning-From-Scratch/blob/main/06%20NaiveBayes/naive_bayes.py\"\n",
        "target=\"_blank\" style=\"text-decoration:none\">Assembly AI</a>"
      ],
      "id": "4440beca-6520-425c-a5df-4629afa51155"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GNaiveBayes:\n",
        "    def fit(self, X,y):\n",
        "        \"\"\"\n",
        "        n_samples: number of observed data n; int;\n",
        "        n_features: number of continueous features d; int;\n",
        "        _classes: unique classes\n",
        "        n_classes: number of unique classes\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # Calculate mean, variance, and prior for each class  \n",
        "        self._mean = np.zeros((n_classes,n_features),dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes,n_features),dtype=np.float64)\n",
        "        self._prior = np.zeros(n_classes,dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y==c]\n",
        "            self._mean[idx,:] = X_c.mean(axis=0)\n",
        "            self._var[idx,:] = X_c.var(axis=0)\n",
        "            self._prior[idx] = X_c.shape[0]/float(n_samples)\n",
        "    \n",
        "    def predict(self,X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # Calculate the posterior probability for each class  \n",
        "        for idx,c in enumerate(self._classes):\n",
        "            prior = np.log(self._prior[idx])\n",
        "            post = np.sum(np.log(self._pdf(idx,x)))\n",
        "            posterior = post + prior\n",
        "            posteriors.append(posterior)\n",
        "        # Return the class with the highest posterior\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "    \n",
        "    def _pdf(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "        numerator = np.exp(-((x-mean)**2)/(2*var))\n",
        "        denominator = np.sqrt(2*np.pi*var)\n",
        "\n",
        "        return numerator/denominator"
      ],
      "id": "10dfef30"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s apply this to the `irish` data set"
      ],
      "id": "e4892ef3-28cb-4167-90ee-e2c8949a2eae"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (Classes)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = pd.Categorical.from_codes(y, data.target_names)\n",
        "df.head()"
      ],
      "id": "a8f843df"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy from custom classifier = 97.78\n",
            "[[19  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  0 13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "\n",
            "\n",
            "Accuracy from sklearn classifier = 97.78\n",
            "[[19  0  0]\n",
            " [ 0 12  1]\n",
            " [ 0  0 13]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  \n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "gnb1 = GNaiveBayes()\n",
        "gnb1.fit(X_train, y_train)\n",
        "pred1 = gnb1.predict(X_test)\n",
        "# Evaluate the model\n",
        "acc1 = accuracy_score(y_test, pred1)\n",
        "\n",
        "gnb2 = GaussianNB()\n",
        "gnb2.fit(X_train, y_train)\n",
        "pred2 = gnb2.predict(X_test)\n",
        "acc2 = accuracy_score(y_test, pred2) \n",
        "\n",
        "print('Accuracy from custom classifier = {:.2f}'.format(acc1*100))\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, pred1))\n",
        "print(classification_report(y_test, pred1))\n",
        "print('\\n')\n",
        "print('Accuracy from sklearn classifier = {:.2f}'.format(acc2*100))\n",
        "print(confusion_matrix(y_test, pred2))\n",
        "print(classification_report(y_test, pred2))"
      ],
      "id": "7a0bf4ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multinomial Naive Bayes"
      ],
      "id": "4a677f35-2dd9-40ee-ad1d-ef0c15c8297f"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0]"
          ]
        }
      ],
      "source": [
        "class MNaiveBayes:\n",
        "    def __init__(self, alpha = 1):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X,y):\n",
        "        \"\"\"\n",
        "        Fit the Multinomial Naive Bayes model to the training data.  \n",
        "        X: input data (n_samples, n_features)\n",
        "        y: target labels (n_samples)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # Initialize and count priors \n",
        "        self._class_feature_count = np.zeros((n_classes, n_features),dtype=np.float64)\n",
        "        self._class_count = np.zeros(n_classes, dtype=np.float64)\n",
        "        self._prior = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx,c in enumerate(self._classes):\n",
        "            X_c = X[y==c]\n",
        "            self._class_feature_count[idx,:] = X_c.sum(axis=0)\n",
        "            self._class_count[idx] = X_c.shape[0]\n",
        "            self._prior[idx] = X_c.shape[0]/float(n_samples)\n",
        "        \n",
        "        # Total count of all features accross all classes \n",
        "        self._total_feature_count = self._class_feature_count.sum(axis=1)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "    \n",
        "    def _predict(self,x):\n",
        "        posteriors = []\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            prior = np.log(self._prior[idx])\n",
        "            likelihood = np.sum(np.log(self._likelihood(idx,x)))\n",
        "            posterior_prob = prior+ likelihood\n",
        "            posteriors.append(posterior_prob)\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "    \n",
        "    def _likelihood(self, class_idx, x):\n",
        "        alpha = self.alpha\n",
        "        V = len(self._class_feature_count[class_idx])\n",
        "        class_feature_count = self._class_feature_count[class_idx]\n",
        "        total_class_count = self._total_feature_count[class_idx]\n",
        "        likelihood = (class_feature_count+alpha)/(total_class_count + alpha * V)\n",
        "\n",
        "        return likelihood**x\n",
        "\n",
        "X = np.array([[2, 1, 0],\n",
        "              [1, 0, 1],\n",
        "              [0, 3, 0],\n",
        "              [2, 2, 1],\n",
        "              [0, 0, 2]])\n",
        "\n",
        "# Corresponding labels (2 classes: 0 and 1)\n",
        "y = np.array([0, 1, 0, 0, 1])\n",
        "\n",
        "# Create and train Multinomial Naive Bayes model\n",
        "model = MNaiveBayes()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict for new sample\n",
        "X_test = np.array([[1, 1, 0], [0, 1, 1]])\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)"
      ],
      "id": "ed9b4d2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pros and Cons of Naive Bayes\n",
        "\n",
        "### Pros:\n",
        "\n",
        "-   **Simplicity**: Easy to implement and computationally efficient.  \n",
        "-   **Fast Training and Prediction**: Naive Bayes is especially fast for\n",
        "    both training and inference, even on large datasets.  \n",
        "-   **Performs Well with Small Data**: Despite its simplicity, Naive\n",
        "    Bayes works well even with relatively small datasets.  \n",
        "-   **Handles Irrelevant Features**: Naive Bayes can often ignore\n",
        "    irrelevant features in the data since the independence assumption\n",
        "    dilutes their influence.  \n",
        "-   **Multi-Class Classification**: Naturally suited for multi-class\n",
        "    classification problems.\n",
        "\n",
        "### Cons:\n",
        "\n",
        "-   **Strong Assumption of Independence**: The assumption that features\n",
        "    are independent is rarely true in real-world data, which can limit\n",
        "    the model’s effectiveness.  \n",
        "-   **Poor Estimation of Probabilities**: When dealing with very small\n",
        "    datasets or unseen feature combinations, Naive Bayes can yield\n",
        "    inaccurate probability estimates.  \n",
        "-   **Zero-Frequency Problem**: If a feature value was not present in\n",
        "    the training data, Naive Bayes will assign zero probability to the\n",
        "    entire class, which can be addressed using Laplace smoothing.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Share on**\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/posts/naivebayes/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/posts/naivebayes/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/posts/naivebayes/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n",
        "\n",
        "</a>"
      ],
      "id": "06e3f0f3-b2ec-4901-aab1-89db68c8069e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script src=\"https://giscus.app/client.js\"\n",
        "        data-repo=\"mrislambd/mrislambd.github.io\" \n",
        "        data-repo-id=\"R_kgDOMV8crA\"\n",
        "        data-category=\"Announcements\"\n",
        "        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n",
        "        data-mapping=\"pathname\"\n",
        "        data-strict=\"0\"\n",
        "        data-reactions-enabled=\"1\"\n",
        "        data-emit-metadata=\"0\"\n",
        "        data-input-position=\"bottom\"\n",
        "        data-theme=\"light\"\n",
        "        data-lang=\"en\"\n",
        "        crossorigin=\"anonymous\"\n",
        "        async>\n",
        "</script>"
      ],
      "id": "432177ed-3df3-43fa-ab8e-edd2a85c1543"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "4781ca35-9829-4acd-ac3f-860a151d061e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script async defer crossorigin=\"anonymous\"\n",
        " src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>"
      ],
      "id": "69a4779e-8814-4ed4-9508-c2fb90968506"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You may also like**"
      ],
      "id": "325ebd35-1e4c-440a-86da-e0ac786bae06"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/hostedtoolcache/Python/3.10.18/x64/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  }
}