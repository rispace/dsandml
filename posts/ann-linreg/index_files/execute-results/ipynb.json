{
  "hash": "a54f01081d888f37f4bad626285f3ed1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Artificial Neural Network (ANN) - Regression\"\ndate: \"2025-03-25\"\nauthor: Rafiq Islam\ncategories: [Data Science, Machine Learning, Artificial Intelligence]\ncitation: true\nsearch: true\nlightbox: true\nimage: lin.png\nlisting: \n    contents: \"/../../posts\"\n    max-items: 3\n    type: grid\n    categories: true\n    date-format: full\n    fields: [image, date, title, author, reading-time]  \nformat: \n    html: default\n    ipynb: default\n    docx: \n      toc: true\n      adsense:\n        enable-ads: false\n    epub:\n      toc: true\n      adsense:\n        enable-ads: false\n    pdf: \n      toc: true\n      pdf-engine: pdflatex\n      adsense:\n        enable-ads: false\n      number-sections: false\n      colorlinks: true\n      cite-method: biblatex\ntoc-depth: 4\n---  \n\n\n\n\n\n\n\n\n\n\n\n\n## Simple Linear Regression Using ANN  \n\nThe simple linear regression equation is given as  \n\n$$\ny_i = \\beta_0+\\beta_1 x_i + \\xi_i = \\sigma (w_0+\\mathbf{x}^T\\mathbf{w})=\\sigma (\\mathbf{x}^T\\mathbf{w})\n$$  \n\n::: {#088a6edf .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/cell-2-output-1.png){}\n:::\n:::\n\n\nThe loss function in this case MSE: Mean Squared Error  \n\n::: {#f9ca3a4b .cell execution_count=2}\n``` {.python .cell-code}\nimport torch \nn = 50 \n# Creating n=50 random X values from the standard normal distribution\nX = torch.randn(n,1) \n# y = mX + c + noise. Here m=1, c = 0, noise = N(0,1)/2\ny = X + torch.randn(n,1)/2 \n\nplt.plot(X,y, 'ro')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/cell-3-output-1.png){}\n:::\n:::\n\n\nNow the model  \n\n::: {#85c953b8 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport torch.nn as nn \n\nANN_regressor = nn.Sequential(\n    nn.Linear(1,1), # Input Layer \n    nn.ReLU(),      # Rectified Linear Unit (ReLU) activation function\n    nn.Linear(1,1)  # Output Layer\n) \nANN_regressor\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nSequential(\n  (0): Linear(in_features=1, out_features=1, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=1, out_features=1, bias=True)\n)\n```\n:::\n:::\n\n\nNext we want to train our model using *Stochastic Gradient Descent* optimizer\n\n::: {#06a39268 .cell execution_count=4}\n``` {.python .cell-code}\nlr = 0.05                              # Learning rate/stepsize\nloss_function = nn.MSELoss()           # MSE loss function  \noptimizer = torch.optim.SGD(           # SGD Optimizer\n    ANN_regressor.parameters(), \n    lr=lr\n)\n\ntraining_epochs = 500                  # Epochs\nlosses = torch.zeros(training_epochs)  # Creating 1D zero vector of size 500\n\n# Train the model \n\nfor epoch in range(training_epochs):\n\n    # forward pass \n    pred = ANN_regressor(X)\n\n    # compute the loss\n    loss = loss_function(pred, y)\n    losses[epoch] = loss\n\n    # back propagation \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\npredictions = ANN_regressor(X)\ntest_loss = (predictions - y).pow(2).mean()\n\nplt.plot(losses.detach())\nplt.plot(training_epochs, test_loss.detach(), 'ro')\nplt.title('Final Loss = %g' %test_loss.item())\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/cell-5-output-1.png){}\n:::\n:::\n\n\nNow let's calculate the predictions  \n\n::: {#caee7b92 .cell execution_count=5}\n``` {.python .cell-code}\nplt.plot(X,y, 'ro', label = 'Data')\nplt.plot(X,predictions.detach(), 'bs', label='Predictions')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/cell-6-output-1.png){}\n:::\n:::\n\n\nPutting all together  \n\n::: {#2d4dc8a5 .cell execution_count=6}\n``` {.python .cell-code}\ndef ann_reg(X,y):\n    model = nn.Sequential(\n        nn.Linear(1,1),\n        nn.ReLU(),\n        nn.Linear(1,1)\n    )\n    loss_function = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n    training_epochs = 500\n\n    losses = torch.zeros(training_epochs)\n\n    for epoch in range(training_epochs):\n        pred = model(X)\n\n        loss = loss_function(pred, y)\n        losses[epoch] = loss \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    return model(X), losses\n\ndef data(m):\n    X = torch.randn(50,1)\n    y = m*X + torch.randn(50,1)/2\n\n    return X, y \n\nslopes = np.linspace(-2,2,21)\n\ntrain = 30\n\nresults = np.zeros((len(slopes), train,2))\n\nfor m in range(len(slopes)):\n    for t in range(train):\n        X,y = data(slopes[m])\n        prediction,loss = ann_reg(X,y)\n        results[m, t, 0] = loss[-1]\n        results[m, t, 1] = np.corrcoef(y.T,prediction.detach().T)[0,1]\n\nresults[np.isnan(results)]=0\n\nfig, ax = plt.subplots(1,2, figsize=(8,4))\n\nax[0].plot(slopes, np.mean(results[:,:,0], axis=1),'ko-')\nax[0].set_xlabel('Slope')\nax[0].set_title('Loss')\n\nax[1].plot(slopes, np.mean(results[:,:,1],axis=1),'ms-')\nax[1].set_xlabel('Slope')\nax[1].set_ylabel('Real vs Predicted correlation')\nax[1].set_title('Model Performance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/cell-7-output-1.png){}\n:::\n:::\n\n\n---  \n\n**Share on**  \n\n::::{.columns}\n:::{.column width=\"33%\"}\n<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/posts/ann-linreg/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n \n\n\n\n\n\n\n\n\n\n\n\n{{< fa brands facebook size=3x >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</a>\n \n:::\n \n:::{.column width=\"33%\"}\n<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/posts/ann-linreg/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n \n\n\n\n\n\n\n\n\n\n\n\n{{< fa brands linkedin size=3x >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</a>\n \n:::\n \n:::{.column width=\"33%\"}\n<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/posts/ann-linreg/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n \n\n\n\n\n\n\n\n\n\n\n\n{{< fa brands twitter size=3x >}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</a>\n \n:::\n::::\n \n<script src=\"https://giscus.app/client.js\"\n        data-repo=\"mrislambd/mrislambd.github.io\" \n        data-repo-id=\"R_kgDOMV8crA\"\n        data-category=\"Announcements\"\n        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n        data-mapping=\"pathname\"\n        data-strict=\"0\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"bottom\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async>\n</script>\n \n<div id=\"fb-root\"></div>\n<script async defer crossorigin=\"anonymous\"\n src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>\n<div class=\"fb-comments\" data-href=\"https://mrislambd.github.io/dsandml/posts/ann-linreg/\" data-width=\"750\" data-numposts=\"5\"></div>  \n\n**You may also like**\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /Users/macpc/Library/CloudStorage/OneDrive-FloridaStateUniversity/QuartoWebPage/WebEnv/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.13.1\n---\n",
    "supporting": [
      "index_files/figure-ipynb"
    ],
    "filters": []
  }
}