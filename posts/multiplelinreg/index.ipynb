{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiple Liear Regression\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-08-29\n",
        "\n",
        "## Multiple Linear Regression\n",
        "\n",
        "The multiple linear regression takes the form  \n",
        "$$\n",
        "y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\n",
        "$$\n",
        "\n",
        "with $\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}$ constants or parameters of\n",
        "the model. In vector notation, $\\vec{\\beta}\\in \\mathbb{R}^{d+1}$,\n",
        "\n",
        "$$\n",
        "\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "For $n$ data points, in matrix algebra notation, we can write\n",
        "$y=X\\vec{\\beta}+\\xi$ where $X\\in \\mathcal{M}_{n\\times (d+1)}$ and\n",
        "$y\\in \\mathbb{R}^{d+1}$ with\n",
        "\n",
        "$$\n",
        "X=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We fit the $n$ data points with the objective to minimize the loss\n",
        "function, mean squared error\n",
        "\n",
        "$$\n",
        "MSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\n",
        "$$\n",
        "\n",
        "## Ordinary Least Square Method"
      ],
      "id": "bb41e2d4-049b-4bf5-8392-5a5ce42139ee"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "d0a66819-a17a-428a-8904-b3fd74ba64c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `scikit-learn` library uses ***Ordinary Least Squares (OLS)***\n",
        "method to find the parameters. This method is good for a simple and\n",
        "relatively smaller dataset. Here is a short note on this method.\n",
        "However, when the dimension is very high and the dataset is bigger,\n",
        "`scikit-learn` uses another method called ***Stochastic Gradient\n",
        "Descent*** for optimization which is discussed in the next section."
      ],
      "id": "ec4e5d20-3d16-4778-9249-ada540ca74ab"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "bbc47e8b-af60-4d01-a4b0-7bfc5762e79e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of OLS is to find the parameter vector $\\hat{\\beta}$ that\n",
        "minimizes the sum of squared errors (SSE) between the observed target\n",
        "values $y$ and the predicted values $\\hat{y}$:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - X_i\\beta)^2\n",
        "$$\n",
        "\n",
        "This can be expressed in matrix form as:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = (y - X\\beta)^T(y - X\\beta)\n",
        "$$\n",
        "\n",
        "To minimize the SSE, let’s first expand the expression:\n",
        "\n",
        "Since $\\beta^T X^T y$ is a scalar (a 1x1 matrix), it is equal to its\n",
        "transpose. That is\n",
        "\n",
        "and therefore,\n",
        "\n",
        "$$\n",
        "\\text{SSE} = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta\n",
        "$$\n",
        "\n",
        "To find the minimum of the SSE, we take the derivative with respect to\n",
        "$\\beta$ and set it to zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{SSE}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0\n",
        "$$\n",
        "\n",
        "Now, solve for $\\beta$:\n",
        "\n",
        "$$\n",
        "X^T X \\beta = X^T y\n",
        "$$\n",
        "\n",
        "To isolate $\\beta$, we multiply both sides by $(X^T X)^{-1}$ (assuming\n",
        "$X^T X$ is invertible):\n",
        "\n",
        "$$\n",
        "\\beta = (X^T X)^{-1} X^T y\n",
        "$$"
      ],
      "id": "9bb60a65-3511-4edd-8351-9bb0d3bff3f8"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "8b3e68be-bc26-44b2-b4fd-3a39da394ad6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector $\\hat{\\beta} = (X^T X)^{-1} X^T y$ gives the estimated\n",
        "coefficients that minimize the sum of squared errors between the\n",
        "observed target values $y$ and the predicted values\n",
        "$\\hat{y} = X\\hat{\\beta}$. This method is exact and works well when\n",
        "$X^T X$ is invertible and the dataset size is manageable. <br> <br> This\n",
        "method is very efficient for small to medium-sized datasets but can\n",
        "become computationally expensive for very large datasets due to the\n",
        "inversion of the matrix $X^TX$."
      ],
      "id": "e606c6b2-6423-42b9-abc5-3ddcc353bab4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "fd36ede9-d419-4ea9-809b-30024949c6f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterative Method\n",
        "\n",
        "### Gradient Descent"
      ],
      "id": "d8c19cf1-5d60-4d17-9a2c-c2721903db77"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "83406e46-e750-4099-bdce-31919e7a28f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img align=\"right\" height=350 width=450 src=\"/posts/multiplelinreg/gradient_descent.gif\" alt=\"Collected from gbhat.com\" style=\"margin-left: 20px; margin-bottom: 20px\"/>\n",
        "<br> GIF Credit:\n",
        "<a href=\"https://gbhat.com/machine_learning/gradient_descent_anim.html\" target=\"_blank\" style=\"text-decoration:none\">gbhat.com</a>  \n",
        "<br> Gradient Descent is an optimization algorithm used to minimize the\n",
        "cost function. The cost function $f(\\beta)$ measures how well a model\n",
        "with parameters $\\beta$ fits the data. The goal is to find the values of\n",
        "$\\beta$ that minimize this cost function. In terms of the iterative\n",
        "method, we want to find $\\beta_{k+1}$ and $\\beta_k$ such that\n",
        "$f(\\beta_{k+1})<f(\\beta_k)$. <br> <br> For a small change in $\\beta$, we\n",
        "can approximate $f(\\beta)$ using Taylor series expansion  \n",
        "$$f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}$$"
      ],
      "id": "37e18937-5108-43ec-83a5-654701f75d42"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "2d7f72bc-fc38-48ef-85f7-a938d8276caa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule for vanilla gradient descent is given by:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $\\beta_k$ is the current estimate of the parameters at iteration\n",
        "    $k$.\n",
        "-   $\\eta$ is the learning rate, a small positive scalar that controls\n",
        "    the step size.\n",
        "-   $\\nabla f(\\beta_k)$ is the gradient of the cost function $f$ with\n",
        "    respect to $\\beta$ at the current point $\\beta_k$."
      ],
      "id": "89f8aea9-c0ad-4707-a09d-d6095896dff3"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "792a8542-5b31-493f-8b4f-11095a538436"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule comes from the idea of moving the parameter vector\n",
        "$\\beta$ in the direction that decreases the cost function the most.\n",
        "\n",
        "1.  **Gradient**: The gradient $\\nabla f(\\beta_k)$ represents the\n",
        "    direction and magnitude of the steepest ascent of the function $f$\n",
        "    at the point $\\beta_k$. Since we want to minimize the function, we\n",
        "    move in the opposite direction of the gradient.\n",
        "\n",
        "2.  **Step Size**: The term $\\eta \\nabla f(\\beta_k)$ scales the gradient\n",
        "    by the learning rate $\\eta$, determining how far we move in that\n",
        "    direction. If $\\eta$ is too large, the algorithm may overshoot the\n",
        "    minimum; if it’s too small, the convergence will be slow.\n",
        "\n",
        "3.  **Iterative Update**: Starting from an initial guess $\\beta_0$, we\n",
        "    repeatedly apply the update rule until the algorithm converges,\n",
        "    meaning that the changes in $\\beta_k$ become negligible, and\n",
        "    $\\beta_k$ is close to the optimal value $\\beta^*$.\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)"
      ],
      "id": "ea1e8c9d-8c1c-49c4-a2ac-2a1d00cbc2eb"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "44fa0f8a-5f1f-4bf4-ab71-d78f3156220b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stochastic Gradient Descent is a variation of the vanilla gradient\n",
        "descent. Instead of computing the gradient using the entire dataset, SGD\n",
        "updates the parameters using only a single data point or a small batch\n",
        "of data points at each iteration. The later one we call it mini batch\n",
        "SGD."
      ],
      "id": "003d7e72-c79b-41bc-b155-c87123923177"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "9ca6141c-93d6-4eb0-993b-1dc79a90e6c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose our cost function is defined as the average over a dataset of\n",
        "size $n$:\n",
        "\n",
        "$$\n",
        "f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Where $f_i(\\beta)$ represents the contribution of the $i$-th data point\n",
        "to the total cost function. The gradient of the cost function with\n",
        "respect to $\\beta$ is:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Vanilla gradient descent would update the parameters as:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Instead of using the entire dataset to compute the gradient, SGD\n",
        "approximates the gradient by using only a single data point (or a small\n",
        "batch). The update rule for SGD is:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $i_k$ is the index of a randomly selected data point at iteration\n",
        "    $k$.\n",
        "-   $\\nabla f_{i_k}(\\beta_k)$ is the gradient of the cost function with\n",
        "    respect to the parameter $\\beta_k$, evaluated only at the data point\n",
        "    indexed by $i_k$.\n",
        "\n",
        "## Python Execution\n",
        "\n",
        "### Synthetic Data"
      ],
      "id": "922929b9-df15-4b42-befc-f6114685623a"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "X=np.random.randn(1000,2)\n",
        "y=3*X[:,0]+2*X[:,1]+1+np.random.randn(1000)"
      ],
      "id": "677336a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So for this project, our known relationship is $y=1+3x_1+2x_2+\\xi$.\n",
        "\n",
        "### Fit the data: Using scikit-learn Library"
      ],
      "id": "e597aa96-7135-44f9-bef4-21534e4a62f7"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlr=LinearRegression()\n",
        "mlr.fit(X,y)\n",
        "coefficients=mlr.coef_.tolist()\n",
        "slope=mlr.intercept_.tolist()"
      ],
      "id": "d18779ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9668 and coefficients\n",
        "$\\beta_1=$ 3.0116, and $\\beta_2=$ 2.0052\n",
        "\n",
        "### Fit the data: Using Custom Library OLS\n",
        "\n",
        "First we create our custom `NewLinearRegression` using the OLS formula\n",
        "above and save this `python` class as `mlreg.py`\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NewLinearRegression:\n",
        "    def __init__(self) -> None:\n",
        "        self.beta = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        X_transpose_X = np.dot(X.transpose(), X)\n",
        "        X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
        "        X_transpose_y = np.dot(X.transpose(), y)\n",
        "        self.beta = np.dot(X_transpose_X_inverse, X_transpose_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return np.dot(X, self.beta)\n",
        "\n",
        "    def coeff_(self):\n",
        "        return self.beta[1:].tolist()\n",
        "\n",
        "    def interceptt_(self):\n",
        "        return self.beta[0].tolist()\n",
        "```\n",
        "\n",
        "Now it’s time to use the new class"
      ],
      "id": "ac524c5e-f0b8-4ffc-944a-16086ffff20e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import NewLinearRegression\n",
        "mlr1 = NewLinearRegression()\n",
        "mlr1.fit(X,y)\n",
        "coefficients1=mlr1.coeff_()\n",
        "slope1=mlr1.interceptt_()"
      ],
      "id": "04550dc3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9668 and coefficients\n",
        "$\\beta_1=$ 3.0116, and $\\beta_2=$ 2.0052\n",
        "\n",
        "### Fit the data: Using Gradient Descent\n",
        "\n",
        "We create the class\n",
        "\n",
        "``` python\n",
        "class GDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, number_of_iteration=1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.number_of_iteration = number_of_iteration\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_of_samples, num_of_features = X.shape\n",
        "        self.weights = np.zeros(num_of_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.number_of_iteration):\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            d_weights = (1 / num_of_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            d_bias = (1 / num_of_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * d_weights\n",
        "            self.bias -= self.learning_rate * d_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_predicted = np.dot(X, self.weights) + self.bias\n",
        "        return y_predicted\n",
        "\n",
        "    def coefff_(self):\n",
        "        return self.weights.tolist()\n",
        "\n",
        "    def intercepttt_(self):\n",
        "        return self.bias\n",
        "```\n",
        "\n",
        "Now we use this similarly as before,"
      ],
      "id": "ef0c7ab8-f1a7-4314-988e-7b6d672c3de0"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import GDLinearRegression\n",
        "mlr2= GDLinearRegression(learning_rate=0.008)\n",
        "mlr2.fit(X,y)\n",
        "coefficients2=mlr2.coefff_()\n",
        "slope2=mlr2.intercepttt_()"
      ],
      "id": "516712e8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ np.float64(0.967) and\n",
        "coefficients $\\beta_1=$ 3.0109, and $\\beta_2=$ 2.0048\n",
        "\n",
        "### Fit the data: Using Stochastic Gradient Descent\n",
        "\n",
        "First we define the class\n",
        "\n",
        "``` python\n",
        "class SGDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, batch_size=1) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.theta = None\n",
        "        self.mse_list = None  # Initialize mse_list as an instance attribute\n",
        "\n",
        "    def _loss_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        mse = (1/num_samples) * np.sum(np.square(y_predicted - y))\n",
        "        return mse\n",
        "\n",
        "    def _gradient_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        grad = (1/num_samples) * X.T.dot(y_predicted - y)\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        num_features = X.shape[1]\n",
        "        self.theta = np.zeros((num_features, 1))\n",
        "\n",
        "        self.mse_list = np.zeros(self.num_iterations)  # Initialize mse_list\n",
        "\n",
        "        for i in range(self.num_iterations):\n",
        "            # Randomly select a batch of data points\n",
        "            indices = np.random.choice(\n",
        "                len(y), size=self.batch_size, replace=False)\n",
        "            X_i = X[indices]\n",
        "            y_i = y[indices].reshape(-1, 1)\n",
        "\n",
        "            # Compute the gradient and update the weights\n",
        "            gradient = self._gradient_function(X_i, y_i, self.theta)\n",
        "            self.theta = self.theta - self.learning_rate * gradient\n",
        "\n",
        "            # Calculate loss for the entire dataset (optional)\n",
        "            self.mse_list[i] = self._loss_function(X, y, self.theta)\n",
        "\n",
        "        return self.theta, self.mse_list\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return X.dot(self.theta)\n",
        "\n",
        "    def coef_(self):\n",
        "        # Return the coefficients (excluding the intercept term)\n",
        "        return self.theta[1:].flatten().tolist()\n",
        "\n",
        "    def intercept_(self):\n",
        "        # Return the intercept term\n",
        "        return self.theta[0].item()\n",
        "\n",
        "    def mse_losses(self):\n",
        "        # Return the mse_list\n",
        "        return self.mse_list.tolist()\n",
        "```\n",
        "\n",
        "Now"
      ],
      "id": "139f581e-1540-46ae-b40f-9386cc6b47d2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlreg import SGDLinearRegression\n",
        "mlr3=SGDLinearRegression(learning_rate=0.01, num_iterations=1000, batch_size=10)\n",
        "theta, _ = mlr3.fit(X, y)"
      ],
      "id": "b570ae73"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ array(\\[0.9817686\\]) and\n",
        "coefficients $\\beta_1=$ array(\\[3.01704758\\]), and $\\beta_2=$\n",
        "array(\\[2.03211565\\])\n",
        "\n",
        "Up next [knn regression](../../posts/knn/index.qmd)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Share on**\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/posts/multiplelinreg/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/posts/multiplelinreg/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/posts/multiplelinreg/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n",
        "\n",
        "</a>"
      ],
      "id": "42298ebc-5aee-4c3f-afe5-950dd35180c5"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script src=\"https://giscus.app/client.js\"\n",
        "        data-repo=\"mrislambd/mrislambd.github.io\" \n",
        "        data-repo-id=\"R_kgDOMV8crA\"\n",
        "        data-category=\"Announcements\"\n",
        "        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n",
        "        data-mapping=\"pathname\"\n",
        "        data-strict=\"0\"\n",
        "        data-reactions-enabled=\"1\"\n",
        "        data-emit-metadata=\"0\"\n",
        "        data-input-position=\"bottom\"\n",
        "        data-theme=\"light\"\n",
        "        data-lang=\"en\"\n",
        "        crossorigin=\"anonymous\"\n",
        "        async>\n",
        "</script>"
      ],
      "id": "cbb6a3b5-88a6-4df4-8cae-be4307aee540"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "2d94ad2d-28e8-42d6-8e38-76699bbe066c"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script async defer crossorigin=\"anonymous\"\n",
        " src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>"
      ],
      "id": "092b3bcb-80ef-45d9-9021-d77c084e5682"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You may also like**"
      ],
      "id": "6d31ad65-15e7-4bb9-abba-0e009184691c"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/hostedtoolcache/Python/3.10.18/x64/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  }
}