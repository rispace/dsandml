{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification: Logistic Regression - A Comprehensive Guide with\n",
        "\n",
        "Mathematical Derivation and Python Code\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-10-07\n",
        "\n",
        "## Introduction"
      ],
      "id": "3d619eb9-b05c-4a5e-b467-9127aa99ce7b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "8fe7f9ec-43c1-46fd-b751-66a0b4670a6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression is a popular classification algorithm used for\n",
        "binary and multi-class classification problems. Unlike Linear\n",
        "Regression, which is used for regression problems, Logistic Regression\n",
        "is used to predict categorical outcomes. In binary classification, the\n",
        "output is either 0 or 1, and the relationship between the input features\n",
        "and the outcome is modeled using a logistic function (also called the\n",
        "sigmoid function)."
      ],
      "id": "7d42af84-ee74-4772-b07a-913fcda09151"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "771eb861-2366-41cd-a01a-e50c64abc64a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Logistic Regression?"
      ],
      "id": "b6f7da79-670d-4781-ac8c-3d8e8db035d4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "50d1b898-f7c7-48ee-8c90-93d7e04ca8a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression is a type of regression analysis used when the\n",
        "dependent variable is categorical. In binary logistic regression, the\n",
        "output can have only two possible outcomes (e.g., 0 or 1, pass or fail,\n",
        "spam or not spam). <br> Logistic Regression works by modeling the\n",
        "probability of an event occurring based on one or more input features.\n",
        "It estimates the probability that a given input belongs to a particular\n",
        "category (0 or 1) using the **logistic function (sigmoid function)**."
      ],
      "id": "8b374c59-e640-4bee-8a2a-0ee5f1352794"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "b7026540-419f-4c20-a288-8f7284f9b0a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Sigmoid Function\n",
        "\n",
        "The sigmoid function maps any real-valued number to a value between 0\n",
        "and 1, making it ideal for modeling probabilities.\n",
        "\n",
        "The sigmoid function is given by the formula:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $z$ is the input to the sigmoid function (in logistic regression,\n",
        "    $z = \\mathbf{x} \\cdot \\theta$)\n",
        "-   $e$ is the base of the natural logarithm\n",
        "\n",
        "The output of the sigmoid function is interpreted as the probability\n",
        "$P(y=1|X)$.\n",
        "\n",
        "## Logistic Regression Model\n",
        "\n",
        "In Logistic Regression, the hypothesis is modeled as:\n",
        "\n",
        "$$\n",
        "h_\\theta(X) = \\frac{1}{1 + e^{-\\theta^T X}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $X$ is the input feature vector\n",
        "-   $\\theta$ is the parameter vector (weights)\n",
        "\n",
        "## Cost Function for Logistic Regression"
      ],
      "id": "d9bcf313-5b7d-47d1-9001-40092ff80d4f"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "374f3daf-e60e-4c9d-85d5-2e576f2164c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike Linear Regression, which uses the Mean Squared Error (MSE) as the\n",
        "cost function, Logistic Regression uses **log loss** or **binary\n",
        "cross-entropy** as the cost function, as the output is binary (0 or 1)."
      ],
      "id": "131d838a-a9f6-46ce-b8d0-aa50815e9c9a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "3e748ace-2b6c-4152-93d1-d49cda7eda26"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, basically we model probability from the given data. In other words,\n",
        "we can write\n",
        "\n",
        "Where, $\\mathbf{\\theta},\\mathbf{x}\\in \\mathbb{R}^{d+1}$ and $d$ is the\n",
        "dimension of the data. For single data vector $\\mathbf{x}$ the binary\n",
        "cross-entropy function can be written as\n",
        "\n",
        "$$\n",
        "l(\\theta) = yp_{\\theta}(\\mathbf{x})+ (1-y)(1-p_{\\theta}(\\mathbf{x}))\n",
        "$$\n",
        "\n",
        "Since we have $n$ of those i.i.d data vectors therefore, we can write\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\prod_{i=1}^{n} \\left(y_ip_{\\theta}(\\mathbf{x_i})+ (1-y_i)(1-p_{\\theta}(\\mathbf{x_i}))\\right)\n",
        "$$\n",
        "\n",
        "Since our goal is to minimize the loss, we need to perform derivatives\n",
        "of the loss function. Therefore, to change from the product form to\n",
        "addition form we take negative log of the above expression\n",
        "\n",
        "For the ease of calculation, let’s rewrite the above equation in terms\n",
        "of $m$ and $b$ where\n",
        "$m\\in \\mathbb{R}^d = (\\theta_1,\\theta_2,\\cdots,\\theta_d)^T$ and\n",
        "$b\\in \\mathbb{R}$.\n",
        "\n",
        "$$\n",
        "\\ell (\\theta) = -\\sum_{i=1}^{n}y_i\\log{p_{m,b}(\\mathbf{x})}+(1-y_i)\\log{(1-p_{m,b}(\\mathbf{x}))}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $n$ is the number of training examples  \n",
        "-   $m$ is the number of features\n",
        "-   $y^{(i)}$ is the true label of the $i^{th}$ example\n",
        "-   $b$ is the bias for the $i^{th}$ example\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "To minimize the cost function and find the optimal values for $\\theta$,\n",
        "we use **gradient descent**. We start from the last form of the loss\n",
        "function and convert this to a form that is easy to take the partial\n",
        "dervivatives.\n",
        "\n",
        "Now we again use the beautiful features of the sigmoid function  \n",
        "\n",
        "Finally, we are ready to take the partial derivatives of the loss\n",
        "function with respect to $m$ and $b$,\n",
        "\n",
        "Using this gradient, we update the parameter vector $\\theta$\n",
        "iteratively:\n",
        "\n",
        "$$\n",
        "\\theta_{j+1} := \\theta_j - \\alpha \\nabla \\ell (\\theta_j)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $\\alpha$ is the learning rate\n",
        "-   $\\nabla \\ell (\\theta_j)$ is the partial derivative of the cost\n",
        "    function with respect to $\\theta_j$ and $$\n",
        "    \\nabla \\ell (\\theta) = \\begin{bmatrix}\\sum_{i=1}^{n} \\hat{y}_i-y_i \\\\\n",
        "    \\sum_{i=1}^{n} x_i(\\hat{y_i}-y_i) \\end{bmatrix}  =\\begin{bmatrix}\\hat{\\mathbf{y}}_i-\\mathbf{y}_i \\\\\n",
        "    \\mathbf{x_i}\\cdot(\\mathbf{\\hat{y_i}}-\\mathbf{y_i}) \\end{bmatrix}= X^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)=X^T(\\sigma(X\\vec{\\theta})-\\vec{y})\n",
        "    $$\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Python Code Implementation from Scratch\n",
        "\n",
        "Here’s how to implement Logistic Regression from scratch in Python. We\n",
        "will use two different forms for our class"
      ],
      "id": "c5bf4dcb-548c-4b2d-b511-50d738190abf"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression1:\n",
        "    def __init__(self, learning_rate = 0.1, n_iterations = 1000):\n",
        "        \"\"\"\n",
        "        Hyper Parameters\n",
        "        - learning_rate: learning rate; float; default 0.01\n",
        "        - n_itearations: number of iterations; int; default 1000\n",
        "        Model Parameters\n",
        "        - weights: weights of the features; float or int\n",
        "        - bias: bias of the model; float or int\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations \n",
        "        self.weights = None\n",
        "        self.bias = None \n",
        "    \n",
        "    def _sigmoid(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def fit(self, X,y):\n",
        "        \"\"\"\n",
        "        n_sample = number of samples in the data set: the value n\n",
        "        n_features = number of features or the dimension of the data set: the value d\n",
        "        \"\"\"\n",
        "        n_sample,n_features = X.shape\n",
        "        self.weights = np.zeros(n_features) \n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            linear = np.dot(X, self.weights) + self.bias\n",
        "            pred = self._sigmoid(linear)\n",
        "\n",
        "            dw = (1/n_sample)* np.dot(X.T,(pred-y))\n",
        "            db = (1/n_sample) * np.sum(pred-y)\n",
        "\n",
        "            self.weights = self.weights - self.learning_rate * dw \n",
        "            self.bias = self.bias - self.learning_rate * db\n",
        "    \n",
        "    def predict(self, X):\n",
        "        linear = np.dot(X, self.weights) + self.bias\n",
        "        predicted_y = self._sigmoid(linear)\n",
        "        class_of_y = [0 if y<=0.5 else 1 for y in predicted_y]\n",
        "        return class_of_y"
      ],
      "id": "fb375ee8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s use this using the `scikit-learn` breast cancer data set."
      ],
      "id": "2cad256c-0273-49b6-b230-0cb0f9039592"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.91"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "b_cancer = load_breast_cancer()\n",
        "X, y = b_cancer.data, b_cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=123, stratify=y, test_size=0.30)\n",
        "\n",
        "clf1 = LogisticRegression1(learning_rate=0.01)\n",
        "clf1.fit(X_train, y_train)\n",
        "predicted_y = clf1.predict(X_test)\n",
        "print(np.round(accuracy_score(predicted_y, y_test),2))"
      ],
      "id": "e8feb6df"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets compare this with the standard `scikit-learn` library"
      ],
      "id": "4e0ceb33-a287-46e8-8d67-0fcfdfd363df"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.96"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf2 = LogisticRegression()\n",
        "clf2.fit(X_train, y_train)\n",
        "predicted_y = clf2.predict(X_test)\n",
        "print(np.round(accuracy_score(predicted_y, y_test),2))"
      ],
      "id": "2bd4e255"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "-   Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*.\n",
        "    Springer.\n",
        "-   Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of\n",
        "    Statistical Learning: Data Mining, Inference, and Prediction*.\n",
        "    Springer.\n",
        "-   Gradient descent is a widely used optimization technique in machine\n",
        "    learning.\n",
        "-   Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*.\n",
        "    MIT Press.\n",
        "-   Nocedal, J., & Wright, S. (2006). *Numerical Optimization* (2nd\n",
        "    ed.). Springer.\n",
        "-   Regularization techniques like L2 (Ridge) and L1 (Lasso) are\n",
        "    commonly used in logistic regression to prevent overfitting.\n",
        "-   Ng, A. (2004). *Feature Selection, L1 vs. L2 Regularization, and\n",
        "    Rotational Invariance*. ICML Proceedings.\n",
        "-   Friedman, J., Hastie, T., & Tibshirani, R. (2010). *Regularization\n",
        "    Paths for Generalized Linear Models via Coordinate Descent*. Journal\n",
        "    of Statistical Software, 33(1), 1-22.\n",
        "-   The extension of logistic regression to multiclass classification\n",
        "    via the softmax function is part of the core material for\n",
        "    understanding classification tasks.\n",
        "-   Murphy, K. P. (2012). *Machine Learning: A Probabilistic\n",
        "    Perspective*. MIT Press.\n",
        "-   Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*.\n",
        "    Springer.\n",
        "-   VanderPlas, J. (2016). *Python Data Science Handbook: Essential\n",
        "    Tools for Working with Data*. O’Reilly Media.\n",
        "-   Raschka, S., & Mirjalili, V. (2017). *Python Machine Learning:\n",
        "    Machine Learning and Deep Learning with Python, scikit-learn, and\n",
        "    TensorFlow 2*. Packt Publishing.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Share on**\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/posts/logreg/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/posts/logreg/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/posts/logreg/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n",
        "\n",
        "</a>"
      ],
      "id": "f934cffe-15d4-4513-b454-c53dc1fc1cec"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script src=\"https://giscus.app/client.js\"\n",
        "        data-repo=\"mrislambd/mrislambd.github.io\" \n",
        "        data-repo-id=\"R_kgDOMV8crA\"\n",
        "        data-category=\"Announcements\"\n",
        "        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n",
        "        data-mapping=\"pathname\"\n",
        "        data-strict=\"0\"\n",
        "        data-reactions-enabled=\"1\"\n",
        "        data-emit-metadata=\"0\"\n",
        "        data-input-position=\"bottom\"\n",
        "        data-theme=\"light\"\n",
        "        data-lang=\"en\"\n",
        "        crossorigin=\"anonymous\"\n",
        "        async>\n",
        "</script>"
      ],
      "id": "de0fcaf2-9830-4299-ab71-2eb4415e5f21"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "f5759328-9600-4ae2-889e-ec57ec6bf44f"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script async defer crossorigin=\"anonymous\"\n",
        " src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>"
      ],
      "id": "fd9c824e-8522-4ced-94e5-ce1c10516c06"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "**You may also like**"
      ],
      "id": "399bd057-3491-4b88-83f5-d3aa9c536239"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/hostedtoolcache/Python/3.10.18/x64/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  }
}